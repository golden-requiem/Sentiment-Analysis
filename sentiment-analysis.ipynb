{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273,"isSourceIdPinned":false}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:02:06.307008Z","iopub.execute_input":"2025-10-03T08:02:06.307302Z","iopub.status.idle":"2025-10-03T08:02:09.199988Z","shell.execute_reply.started":"2025-10-03T08:02:06.307272Z","shell.execute_reply":"2025-10-03T08:02:09.198666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\n# data processing, modeling, elavuation\n# Logistic Regression model (linear classifier for binary/multiclass tasks)\nfrom sklearn.linear_model import LogisticRegression  \n# Random Forest classifier (ensemble of decision trees using bagging + randomness)\nfrom sklearn.ensemble import RandomForestClassifier  \n# Tools for splitting data, doing cross-validation, and tuning hyperparameters\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV  \n# Multinomial Naive Bayes classifier (good for text data, word counts, frequencies)\nfrom sklearn.naive_bayes import MultinomialNB  \n# Metrics for evaluating model predictions (confusion matrix, reports, accuracy)\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score  \n# Pipeline for chaining preprocessing steps with model training\nfrom sklearn.pipeline import Pipeline  \n# XGBoost classifier (boosting algorithm\nfrom xgboost import XGBClassifier  \n# LightGBM classifier (gradient boosting framework optimized for speed and large datasets)\nfrom lightgbm import LGBMClassifier  \n# CatBoost classifier (gradient boosting library optimized for categorical features)\nfrom catboost import CatBoostClassifier  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:12:13.867888Z","iopub.execute_input":"2025-10-03T08:12:13.868272Z","iopub.status.idle":"2025-10-03T08:12:22.252546Z","shell.execute_reply.started":"2025-10-03T08:12:13.868247Z","shell.execute_reply":"2025-10-03T08:12:22.251259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:46:41.483977Z","iopub.execute_input":"2025-10-03T08:46:41.484359Z","iopub.status.idle":"2025-10-03T08:46:41.490192Z","shell.execute_reply.started":"2025-10-03T08:46:41.484334Z","shell.execute_reply":"2025-10-03T08:46:41.488362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.corpus import stopwords\n# text processing, sentiment analysis, and NLP tasks\nfrom textblob import Word, TextBlob\n# vader for scoring text positivity/negativity\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n# embedding\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# word base roots\nfrom nltk.stem import WordNetLemmatizer\nimport spacy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:19:26.312943Z","iopub.execute_input":"2025-10-03T08:19:26.314002Z","iopub.status.idle":"2025-10-03T08:19:26.319455Z","shell.execute_reply.started":"2025-10-03T08:19:26.313965Z","shell.execute_reply":"2025-10-03T08:19:26.318273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sea\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n# fully connected nn layer\nfrom tensorflow.keras.layers import Dense","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:22:18.172098Z","iopub.execute_input":"2025-10-03T08:22:18.172463Z","iopub.status.idle":"2025-10-03T08:22:18.180417Z","shell.execute_reply.started":"2025-10-03T08:22:18.172441Z","shell.execute_reply":"2025-10-03T08:22:18.179197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:22:33.593972Z","iopub.execute_input":"2025-10-03T08:22:33.595103Z","iopub.status.idle":"2025-10-03T08:22:33.599926Z","shell.execute_reply.started":"2025-10-03T08:22:33.595068Z","shell.execute_reply":"2025-10-03T08:22:33.598623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# all columns when printing a DataFrame \npd.set_option('display.max_columns', None)\n# Donâ€™t limit line width when displaying DataFrames in the console\npd.set_option('display.width', None)\npd.set_option('display.max_rows', 55)\n# Format floating-point numbers to 3 decimal places when displaying\npd.set_option('display.float_format', lambda x: '%.3f' % x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:24:48.027517Z","iopub.execute_input":"2025-10-03T08:24:48.027850Z","iopub.status.idle":"2025-10-03T08:24:48.033894Z","shell.execute_reply.started":"2025-10-03T08:24:48.027827Z","shell.execute_reply":"2025-10-03T08:24:48.033000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:02:39.253342Z","iopub.execute_input":"2025-10-03T08:02:39.253711Z","iopub.status.idle":"2025-10-03T08:02:44.839163Z","shell.execute_reply.started":"2025-10-03T08:02:39.253687Z","shell.execute_reply":"2025-10-03T08:02:44.838035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentiment-analysis-for-mental-health/Combined Data.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:26:28.851928Z","iopub.execute_input":"2025-10-03T08:26:28.852292Z","iopub.status.idle":"2025-10-03T08:26:29.812965Z","shell.execute_reply.started":"2025-10-03T08:26:28.852269Z","shell.execute_reply":"2025-10-03T08:26:29.810932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:26:54.608305Z","iopub.execute_input":"2025-10-03T08:26:54.608647Z","iopub.status.idle":"2025-10-03T08:26:54.652953Z","shell.execute_reply.started":"2025-10-03T08:26:54.608625Z","shell.execute_reply":"2025-10-03T08:26:54.651285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace any mising value with empty string\ndf['statement'] = df['statement'].fillna('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:28:08.065798Z","iopub.execute_input":"2025-10-03T08:28:08.066165Z","iopub.status.idle":"2025-10-03T08:28:08.084565Z","shell.execute_reply.started":"2025-10-03T08:28:08.066101Z","shell.execute_reply":"2025-10-03T08:28:08.083161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:28:15.946717Z","iopub.execute_input":"2025-10-03T08:28:15.947039Z","iopub.status.idle":"2025-10-03T08:28:15.959385Z","shell.execute_reply.started":"2025-10-03T08:28:15.947017Z","shell.execute_reply":"2025-10-03T08:28:15.958215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop column 9(axis =1 drop column not row)\ndf.drop(['Unnamed: 0'], axis = 1, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:29:45.117964Z","iopub.execute_input":"2025-10-03T08:29:45.118745Z","iopub.status.idle":"2025-10-03T08:29:45.130798Z","shell.execute_reply.started":"2025-10-03T08:29:45.118712Z","shell.execute_reply":"2025-10-03T08:29:45.129432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['status'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:30:23.554838Z","iopub.execute_input":"2025-10-03T08:30:23.555290Z","iopub.status.idle":"2025-10-03T08:30:23.566594Z","shell.execute_reply.started":"2025-10-03T08:30:23.555262Z","shell.execute_reply":"2025-10-03T08:30:23.565451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['status'] == 'Anxiety']['statement'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:31:40.369076Z","iopub.execute_input":"2025-10-03T08:31:40.369489Z","iopub.status.idle":"2025-10-03T08:31:40.385004Z","shell.execute_reply.started":"2025-10-03T08:31:40.369463Z","shell.execute_reply":"2025-10-03T08:31:40.383589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Converts text labels â†’ numbers\ndf['status'] = df['status'].map({'Anxiety': 0, 'Normal':1, 'Depression': 2, 'Suicidal': 3, 'Stress': 4, 'Bipolar': 5, \"Personality disorder\": 6})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:33:32.221679Z","iopub.execute_input":"2025-10-03T08:33:32.222013Z","iopub.status.idle":"2025-10-03T08:33:32.233729Z","shell.execute_reply.started":"2025-10-03T08:33:32.221991Z","shell.execute_reply":"2025-10-03T08:33:32.232438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    #normalizing case folding\n    text = text.lower() #everything to lower vase\n    text = text.replace('\\n', \" \")# remove newlien\n    text = text.translate(str.maketrans('', '', string.punctuation)) #remove punctuations\n    text = re.sub(r'\\d+', '', text) #remove digits\n    text = re.sub(r'\\[.*?\\]', '', text) # remove parenthesis\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # remove url\n    text = re.sub(r'<.*?>', '', text) #remove html tags\n    text = re.sub(r'\\w*\\d\\w*', '', text) #remove words with numbers\n    return text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:47:57.589212Z","iopub.execute_input":"2025-10-03T08:47:57.589532Z","iopub.status.idle":"2025-10-03T08:47:57.596987Z","shell.execute_reply.started":"2025-10-03T08:47:57.589511Z","shell.execute_reply":"2025-10-03T08:47:57.595663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['statement'] = df['statement'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:47:59.778964Z","iopub.execute_input":"2025-10-03T08:47:59.780259Z","iopub.status.idle":"2025-10-03T08:48:14.086819Z","shell.execute_reply.started":"2025-10-03T08:47:59.780222Z","shell.execute_reply":"2025-10-03T08:48:14.085618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_stopwords(text):\n    stop_words = stopwords.words('english')\n    #remove stop words in each row\n    text = text.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop_words))\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:51:26.464541Z","iopub.execute_input":"2025-10-03T08:51:26.464913Z","iopub.status.idle":"2025-10-03T08:51:26.471923Z","shell.execute_reply.started":"2025-10-03T08:51:26.464886Z","shell.execute_reply":"2025-10-03T08:51:26.470726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['statement'] = remove_stopwords(df['statement'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:51:28.279529Z","iopub.execute_input":"2025-10-03T08:51:28.279825Z","iopub.status.idle":"2025-10-03T08:51:38.610794Z","shell.execute_reply.started":"2025-10-03T08:51:28.279805Z","shell.execute_reply":"2025-10-03T08:51:38.609754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d = (' '.join(df['statement']).split())\nd[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:57:44.698739Z","iopub.execute_input":"2025-10-03T08:57:44.699156Z","iopub.status.idle":"2025-10-03T08:57:45.144405Z","shell.execute_reply.started":"2025-10-03T08:57:44.699103Z","shell.execute_reply":"2025-10-03T08:57:45.143193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# least common 1000 words\ndelete = pd.Series(' '.join(df['statement']).split()).value_counts()[-1000:]\n# removes least common words from df['statement']\ndf['statement'] = df['statement'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:07:45.658046Z","iopub.execute_input":"2025-10-03T09:07:45.658382Z","iopub.status.idle":"2025-10-03T09:07:53.747657Z","shell.execute_reply.started":"2025-10-03T09:07:45.658360Z","shell.execute_reply":"2025-10-03T09:07:53.746306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\ndef lemmatize_sentence(sentence):\n    doc = nlp(sentence) # text to tokens, word to base root\n    return \" \".join([token.lemma_ for token in doc]) # loop through each token and get base root","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:10:58.424017Z","iopub.execute_input":"2025-10-03T09:10:58.424382Z","iopub.status.idle":"2025-10-03T09:10:59.593881Z","shell.execute_reply.started":"2025-10-03T09:10:58.424359Z","shell.execute_reply":"2025-10-03T09:10:59.592526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f = df['statement'].apply(lemmatize_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:12:05.976505Z","iopub.execute_input":"2025-10-03T09:12:05.976982Z","iopub.status.idle":"2025-10-03T09:27:36.610251Z","shell.execute_reply.started":"2025-10-03T09:12:05.976946Z","shell.execute_reply":"2025-10-03T09:27:36.608502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['statement'] = f\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:28:58.678074Z","iopub.execute_input":"2025-10-03T09:28:58.678533Z","iopub.status.idle":"2025-10-03T09:28:58.692684Z","shell.execute_reply.started":"2025-10-03T09:28:58.678501Z","shell.execute_reply":"2025-10-03T09:28:58.690764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()\n# if its 0 for both statement and status, then there are no None value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:30:27.923694Z","iopub.execute_input":"2025-10-03T09:30:27.924021Z","iopub.status.idle":"2025-10-03T09:30:27.939754Z","shell.execute_reply.started":"2025-10-03T09:30:27.923999Z","shell.execute_reply":"2025-10-03T09:30:27.938652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = df['status'] # output\nx = df.drop('status', axis=1) # drop output column and axis=1 drops only colums","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:31:16.210954Z","iopub.execute_input":"2025-10-03T09:31:16.211307Z","iopub.status.idle":"2025-10-03T09:31:16.220383Z","shell.execute_reply.started":"2025-10-03T09:31:16.211284Z","shell.execute_reply":"2025-10-03T09:31:16.219030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# splitting data into test and train\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:33:04.071324Z","iopub.execute_input":"2025-10-03T09:33:04.072849Z","iopub.status.idle":"2025-10-03T09:33:04.089363Z","shell.execute_reply.started":"2025-10-03T09:33:04.072803Z","shell.execute_reply":"2025-10-03T09:33:04.087967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:33:18.565604Z","iopub.execute_input":"2025-10-03T09:33:18.565937Z","iopub.status.idle":"2025-10-03T09:33:18.579510Z","shell.execute_reply.started":"2025-10-03T09:33:18.565915Z","shell.execute_reply":"2025-10-03T09:33:18.577981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vecotrize using tfidf\ndef tfidf_vec_boost(model, text_col, max_df, max_features, ngram_range, n_estimators, max_depth=3, learning_rate=0.03, verbose = True):\n    # textcol = name of columns\n    # max_df = max document frequency threshold for tfidf vecotrizer (ignore veyr comon words)\n    # max_features: unique words (top most important words)\n    # ngram_range = captures context and meaning single words might miss\n    # estimators = number of trees built for random forest and number of boosting rounds for boosting algos\n    # max_depth = how deep each tree can go (complexity)\n    \n    # vectorize data\n    vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features, ngram_range=ngram_range)\n    x_train_vector = vectorizer.fit_transform(x_train[text_col])\n    # initialize and train classifier\n    classifier = model(n_estimators=n_estimators, max_depth = max_depth, learning_rate=learning_rate, verbose=verbose)\n    classifier.fit(x_train_vector, y_train)\n    #transform test data using trained vectorizer\n    x_test_vector = vectorizer.transform(x_test[text_col])\n    #predict test labels\n    y_prediction = classifier.predict(x_test_vector)\n    #pritn classification report\n    print(classification_report(y_test, y_prediction))\n    #compute and plot confusion matrix\n    cm = confusion_matrix(y_test, y_prediction)\n    labels = y_test.unique().tolist()\n    plt.figure(figsize=(8, 6))\n    sea.heatmap(cm, annot=True, cmap='viridis', fmt='d', xticklabels=labels, yticklabels=labels)\n    #cm = 2d confusion matrix\n    # annot = values in each matrix cell\n    # fmt = d (d for decimal)\n    plt.xlabel(\"Prediction\")\n    plt.ylabel(\"True\")\n    plt.title('Confusion matrix')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:41:33.708282Z","iopub.execute_input":"2025-10-03T13:41:33.708920Z","iopub.status.idle":"2025-10-03T13:41:33.717298Z","shell.execute_reply.started":"2025-10-03T13:41:33.708893Z","shell.execute_reply":"2025-10-03T13:41:33.716093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scikit pipeline\ndef pipe_boosting(vectorizer, classifier, x_grid, y_grid):\n    \"\"\"\n    pipeline = Pipeline([\n        ('vectorizer', vectorizer), ('classifier', classifier)\n    ])\"\"\"\n    pipe = Pipeline(\n    steps=[('vectorizer', TfidfVectorizer()),\n           ('classifier', LGBMClassifier(force_col_wise=True, verbose=1))],\n    memory='cache_dir'  # create this folder once)\n    hyper_params = {\n    'vectorizer__ngram_range': [(1,2)],          # keep (1,3) for later\n    'vectorizer__max_df': [0.9, 1.0],\n    'vectorizer__min_df': [3, 5],                # drop very rare terms\n    'vectorizer__max_features': [1000, 2000],    # cap vocabulary\n    'classifier__n_estimators': [100, 200],\n    'classifier__learning_rate': [0.05, 0.07]}\n    # gridsearchcv tries all hyperparamters given\n    # cv = cross validation, cv = 5 - Each model is trained on 4 parts and validated on the remaining 1 part, repeated 5 times\\\n    # n_jobs = #of cores to use (-1 = all cores)\n    #grid_search = GridSearchCV(pipeline, hyper_params, cv=3, n_jobs=-1, verbose=1)\n    #grid_search.fit(x_grid, y_grid)\n    search = RandomizedSearchCV(pipeline, hyper_params, n_iter=12, cv=3, n_jobs=-1, verbose=1, random_state=42)\n    search.fit(x_train['statement'], y_train)\n    print(\"Optimal hyperparamters:\", search.best_params_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:20:55.547181Z","iopub.execute_input":"2025-10-03T13:20:55.547566Z","iopub.status.idle":"2025-10-03T13:20:55.556763Z","shell.execute_reply.started":"2025-10-03T13:20:55.547541Z","shell.execute_reply":"2025-10-03T13:20:55.554712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_new_text(text, c_vectorizer, model):\n    #predict category of text using vecotrizer and model\n    labels = {\n        0: 'Anxiety',\n        1: 'Normal',\n        2: 'Depression',\n        3: 'Suicidal',\n        4: 'Stress',\n        5: 'Bipolar',\n        6: 'Personality disorder'\n    }\n    # embedding\n    text_vec = c_vectorizer.transform([text])\n    # predict\n    prediction = model.predict(text_vec)[0]\n    # map predicted index to label\n    prediction_label = labels[prediction]\n    return {prediction_label: prediction}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:00:55.108531Z","iopub.execute_input":"2025-10-03T14:00:55.108884Z","iopub.status.idle":"2025-10-03T14:00:55.116183Z","shell.execute_reply.started":"2025-10-03T14:00:55.108861Z","shell.execute_reply":"2025-10-03T14:00:55.114635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tfidf_vec_boost(XGBClassifier, 'statement', 1.0, 2000, (1, 3), 200, 5, 0.07, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:33:45.801625Z","iopub.execute_input":"2025-10-03T11:33:45.801929Z","iopub.status.idle":"2025-10-03T11:39:20.797435Z","shell.execute_reply.started":"2025-10-03T11:33:45.801907Z","shell.execute_reply":"2025-10-03T11:39:20.795851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe_boosting(vectorizer = TfidfVectorizer(), classifier = LGBMClassifier(force_col_wise=True, verbose=1),\n             x_grid = x_train['statement'], y_grid=y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe_boosting(\n    vectorizer=TfidfVectorizer(), \n    classifier=LGBMClassifier(\n        n_estimators=200,\n        max_depth=5,\n        learning_rate=0.07,\n        force_col_wise=True,\n        verbose=1   # change to 1 if you want progress printed\n    ),\n    x_grid=x_train['statement'],\n    y_grid=y_train\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Pick one reasonable config of your pipeline\nsample_pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=2000, ngram_range=(1,2))),\n    ('clf', LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.07, force_col_wise=True, verbose=-1))\n])\n\n# Time a single fit\nt0 = time.time()\nsample_pipeline.fit(x_train['statement'][:5000], y_train[:5000])  # subset to make it faster\nelapsed = time.time() - t0\n\nprint(f\"Single fit on subset took ~{elapsed:.2f} seconds\")\n\n# Rough estimate for full gridsearch\ntotal_fits = 42  # from \"Fitting 3 folds for each of 14 candidates\"\ncores = 12       # you said you have 12 cores\napprox_time = (elapsed * 42 / cores)\n\nprint(f\"Estimated total GridSearch time: ~{approx_time:.2f} seconds (~{approx_time/60:.1f} minutes)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:11:41.528773Z","iopub.execute_input":"2025-10-03T13:11:41.529105Z","iopub.status.idle":"2025-10-03T13:11:50.587325Z","shell.execute_reply.started":"2025-10-03T13:11:41.529082Z","shell.execute_reply":"2025-10-03T13:11:50.586175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params = {'learning_rate': .03, 'n_estimators': 200, 'max_df': 1.0, 'max_features': 2000, 'ngram_range': (1,3)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:33:52.177646Z","iopub.execute_input":"2025-10-03T13:33:52.177963Z","iopub.status.idle":"2025-10-03T13:33:52.183228Z","shell.execute_reply.started":"2025-10-03T13:33:52.177940Z","shell.execute_reply":"2025-10-03T13:33:52.182289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tfidf_vec_boost(LGBMClassifier, 'statement', **best_params, verbose=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:38:03.917579Z","iopub.execute_input":"2025-10-03T13:38:03.917918Z","iopub.status.idle":"2025-10-03T13:38:57.623756Z","shell.execute_reply.started":"2025-10-03T13:38:03.917896Z","shell.execute_reply":"2025-10-03T13:38:57.622663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# vectorize once\nvectorizer = TfidfVectorizer(max_df=1.0, max_features=2000, ngram_range=(1,2))\nx_train_vector = vectorizer.fit_transform(x_train['statement'])\nx_test_vector  = vectorizer.transform(x_test['statement'])\n\n# train model\nmodel = XGBClassifier(**best_params, verbose=True)\nmodel.fit(x_train_vector, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:50:25.986680Z","iopub.execute_input":"2025-10-03T13:50:25.987027Z","iopub.status.idle":"2025-10-03T13:59:00.677860Z","shell.execute_reply.started":"2025-10-03T13:50:25.987002Z","shell.execute_reply":"2025-10-03T13:59:00.676724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = df['statement'].sample(2)\ntext_index = text.index\nprint(text.index)\ntext = text.values[0]\ntext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:01:16.547572Z","iopub.execute_input":"2025-10-03T14:01:16.547961Z","iopub.status.idle":"2025-10-03T14:01:16.562386Z","shell.execute_reply.started":"2025-10-03T14:01:16.547936Z","shell.execute_reply":"2025-10-03T14:01:16.559711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_new_text(text, vectorizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:01:19.853951Z","iopub.execute_input":"2025-10-03T14:01:19.854872Z","iopub.status.idle":"2025-10-03T14:01:19.865429Z","shell.execute_reply.started":"2025-10-03T14:01:19.854834Z","shell.execute_reply":"2025-10-03T14:01:19.864331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[text_index]['status']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:01:39.099010Z","iopub.execute_input":"2025-10-03T14:01:39.099594Z","iopub.status.idle":"2025-10-03T14:01:39.113672Z","shell.execute_reply.started":"2025-10-03T14:01:39.099559Z","shell.execute_reply":"2025-10-03T14:01:39.112590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}